# -*- coding: utf-8 -*-
"""hari reddy-summarization using T5(HuggingFace).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zoHfkyDPvWEdnibOQ5-TICscifqyjKqa
"""

!pip install transformers==4.20.0
!pip install keras_nlp==0.3.0
!pip install datasets
!pip install huggingface-hub
!pip install nltk
!pip install rouge-score

import os
import logging

import nltk
import numpy as np
import tensorflow as tf
from tensorflow import keras

# Only log error messages
tf.get_logger().setLevel(logging.ERROR)

os.environ["TOKENIZERS_PARALLELISM"] = "false"

import keras_nlp

from transformers import AutoTokenizer
from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq

from datasets  import load_dataset

model_path = "t5-small"
data_path = "EdinburghNLP/xsum"

dataset = load_dataset(data_path, split='train')

dataset

dataset[0]

tokenizer = AutoTokenizer.from_pretrained(model_path)

tokenizer(dataset['document'][1], truncation=True ,return_tensors='tf')

final_dataset = dataset.train_test_split(
    train_size=0.25, test_size=0.05
)

final_dataset

def tokenization(examples):
    inputs = ['summarize: ' + doc for doc in examples["document"]]
    tok = tokenizer(inputs, truncation=True, padding = True, return_tensors='tf')

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples['summary'],truncation=True, padding = True, return_tensors='tf')

    tok['labels'] = labels['input_ids']

    return {'input_ids': tok['input_ids'].numpy(),
            'labels': tok['labels'].numpy(),
            'attention_mask': tok['attention_mask'].numpy()}

tokenized_dataset = final_dataset.map(tokenization, batched=True)

tokenized_dataset

processed_data = tokenized_dataset.remove_columns(['document','summary', 'id'])

processed_data

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_path)

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model= model, return_tensors="tf")

train_dataset = model.prepare_tf_dataset(processed_data['train'], batch_size=32, tokenizer= tokenizer, collate_fn=data_collator, shuffle=True, drop_remainder=True)

test_dataset = model.prepare_tf_dataset(processed_data['test'], batch_size=32, tokenizer= tokenizer, collate_fn=data_collator, shuffle=False, drop_remainder=True)

optimizer = keras.optimizers.Adam(learning_rate=2e-5)
model.compile(optimizer=optimizer)

rouge_l = keras_nlp.metrics.RougeL()

def metric_fn(eval_predictions):
    predictions, labels = eval_predictions
    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    for label in labels:
        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge_l(decoded_labels, decoded_predictions)
    result = {"RougeL": result["f1_score"]}

    return result

from transformers.keras_callbacks import KerasMetricCallback

metric_callback = KerasMetricCallback(
    metric_fn, eval_dataset=test_dataset)

callbacks = [metric_callback]

#model.fit(train_dataset, validation_data=test_dataset, epochs=5, verbose=True)

#model.save_weights('summarized_model')

#tokenizer.save_vocabulary("/kaggle/working/")

from transformers import pipeline

pipe = pipeline("summarization", model=model, tokenizer=tokenizer, framework="tf")

dataset[1]

pipe(dataset['document'][1])

# Assuming 'pipe' is your summarization pipeline
document_to_summarize = "A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel. As they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames. One of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland. The driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed. Both groups have organised replacement coaches and will begin their tour of the north coast later than they had planned. Police have appealed for information about the attack. Insp David Gibson said: 'It appears as though the fire started under one of the buses before spreading to the second. While the exact cause is still under investigation, it is thought that the fire was started deliberately.'"

# Use the summarization pipeline with a smaller max_length
summary = pipe(document_to_summarize, max_length=500)

# Print or inspect the generated summary
print(summary)